{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21679cd3-14c4-4b6f-aa2c-183a1a604d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index\n",
    "pip install langchain\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b63f7cc-9fdb-4c1b-8cd2-e9a66db48f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c622135b-48d3-4a8a-b899-39943a8441d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex\n",
    "from llama_index import LLMPredictor, PromptHelper\n",
    "from llama_index import download_loader\n",
    "import openai\n",
    "import json\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b8004-3921-47f6-baf1-a8288a7bdd3a",
   "metadata": {},
   "source": [
    "# Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e29a2a5-1edd-4010-94ee-54dc6314efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load whatever data is in the folder\n",
    "documents = SimpleDirectoryReader('./data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6177672-a816-42f1-bd7d-50ba54a4c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build index\n",
    "index = GPTVectorStoreIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55abbc7-801e-453d-82d5-799b17592556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7f11d76679d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6214963b-18cd-4365-abe7-5514079d712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLama is a state-of-the-art large language model designed to help researchers advance their work in this subfield of AI. It is a foundation model that is trained on a large set of unlabeled data, making it ideal for fine-tuning for a variety of tasks. It is available in several sizes (7B, 13B, 33B, and 65B parameters) and is released under a noncommercial license focused on research use cases.\n"
     ]
    }
   ],
   "source": [
    "#query the engine\n",
    "engine = index.as_query_engine()\n",
    "response = engine.query(\"What is LLama?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066cfef-69ae-470f-9049-258f2b247fdd",
   "metadata": {},
   "source": [
    "# Random Wiki\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9970ce-4295-4e8b-845f-ef4f23e447da",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikireader = download_loader('WikipediaReader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead5ef71-7a2d-4a6e-bf3e-12f4986363cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = wikireader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2955902-a563-47ef-9a94-b80027bd3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Yinchuan\n",
    "\n",
    "wikidocs = loader.load_data(pages=['Yinchuan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c51f7cd-edf0-4e72-92b7-8c3babe773b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_index = GPTVectorStoreIndex(wikidocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "484ac8e3-3c69-46b1-b531-fb7fba7934a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yinchuan is a city in the Ningxia Hui Autonomous Region of China.\n"
     ]
    }
   ],
   "source": [
    "engine = index.as_query_engine()\n",
    "response = engine.query(\"Where is Yinchuan?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e86b3a-8c42-4810-8b78-3a6ad095b601",
   "metadata": {},
   "source": [
    "# CSR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0208da5-774c-4560-a687-8c37ffba4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader('./data2').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a56a147-24d8-4710-a521-9a3c571ee377",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92426764-59f0-4eba-9e90-f8532f890538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec8c2a5-ba84-4a27-b4bc-81251f091d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Git and GitHub can be used to version control, collaborate on projects, share code, contribute to open source projects, automate workflows, monitor performance, streamline processes, narrow searches, increase developer productivity, and host discussions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "engine = index.as_query_engine()\n",
    "response = engine.query(\"What can git or github do??\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bd5e9-3129-4be5-8232-87b9bfbdd2d5",
   "metadata": {},
   "source": [
    "# Video Transcriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085e21e6-8509-4709-9009-fc40dd11eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "YTtranscriptreader = download_loader(\"YoutubeTranscriptReader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4906161-675c-4fbd-afc2-db3963ff2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YTtranscriptreader()\n",
    "psu = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=diUOdC2tYzM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92c3c40-4e4b-4f6f-8b61-309bf9a43a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex(psu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76396b0-bd13-4d17-9cee-0631e30fad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It depends on your needs and budget. If you want the latest features, such as contact posters, check-in, FaceTime video mail, AirTag sharing, Live Voicemail, Swipe to Reply, Check-In, Audio Message Transcription, and more, then you should buy the newest iPhone model. If you don't need all the latest features, you can buy an older model and save some money.\n"
     ]
    }
   ],
   "source": [
    "engine = index.as_query_engine()\n",
    "response = engine.query(\"What kind of iphone should I buy\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b512b-6f89-47c5-9020-0cda172b4505",
   "metadata": {},
   "source": [
    "# Chatbot Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674e3e72-b937-44e5-8a23-e572829e115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        engine = index.as_query_engine()\n",
    "        response = engine.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db52c1c4-2b57-4898-b80f-763e5597575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data2').load_data()\n",
    "index = GPTVectorStoreIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd85c51-d8fe-4196-9211-6545135ca0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n",
      "Lo siento, no entiendo la pregunta. ¿Podría ser más específico?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "bot = Chatbot(\"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\", index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802011a-be18-4bb2-bcae-3b5b2a8711f1",
   "metadata": {},
   "source": [
    "# KEYS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead44053-baa9-42a8-a16a-df80de10e7b1",
   "metadata": {},
   "source": [
    "NEW: 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\n",
    "\n",
    "OLD: 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10caa868-6a9a-4d0c-9566-b0b29533e3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
